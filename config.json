{
    "hidden_size": 256,
    "intermediate_size": 1024,
    "num_hidden_layers": 4,
    "max_position_embeddings": 256,
    "tie_word_embeddings": true,

    "num_attention_heads": 4,
    "num_key_value_heads": 2,
    "head_dim": 64,

    "dropout_p": 0.1,
    "seed": 1728,

    "vocab_size": 8192,
    "special_tokens": [
        "<|startoftext|>",
        "<|endoftext|>"
    ],

    "batch_size": 4
}

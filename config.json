{
    "hidden_size": 256,
    "intermediate_size": 1024,
    "num_hidden_layers": 4,
    "max_position_embeddings": 1024,
    "tie_word_embeddings": true,

    "num_attention_heads": 4,
    "num_key_value_heads": 2,
    "head_dim": 64,

    "dropout_p": 0.1,
    "seed": 1728,

    "vocab_size": 8192,
    "special_tokens": [
        "<|startoftext|>",
        "<|endoftext|>"
    ],

    "batch_size": 16,
    "block_size": 512,
    "learning_rate": 5e-4,
    "eval_iters": 50,
    "eval_interval": 500,
    "max_iters": 35000,

    "temperature": 0.7
}

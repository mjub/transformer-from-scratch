{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5344ee6",
   "metadata": {},
   "source": [
    "# Anatomy of a GPT\n",
    "\n",
    "*An investigation into the internal states of the model. Spoiler: The only thing falling here is the Loss Curve*\n",
    "\n",
    "![Anatomy of a GPT](../assets/anatomy-of-a-gpt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013db42",
   "metadata": {},
   "source": [
    "We start by loading the base `config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9765ee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(hidden_size=256,\n",
       "          intermediate_size=1024,\n",
       "          num_hidden_layers=4,\n",
       "          max_position_embeddings=256,\n",
       "          tie_word_embeddings=True,\n",
       "          num_attention_heads=4,\n",
       "          num_key_value_heads=2,\n",
       "          head_dim=64,\n",
       "          dropout_p=0.1,\n",
       "          seed=1728,\n",
       "          vocab_size=8192,\n",
       "          special_tokens=['<|startoftext|>', '<|endoftext|>'],\n",
       "          batch_size=4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import types\n",
    "\n",
    "with open(\"../config.json\") as fd:\n",
    "    config = json.load(fd, object_hook=lambda d: types.SimpleNamespace(**d))\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfcb71",
   "metadata": {},
   "source": [
    "We then load the model and check its internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2b24237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Transformer                                   [4, 256, 8192]            --\n",
       "├─Embedding: 1-1                              [4, 256, 256]             2,097,152\n",
       "├─Embedding: 1-2                              [256, 256]                65,536\n",
       "├─Dropout: 1-3                                [4, 256, 256]             --\n",
       "├─ModuleList: 1-4                             --                        --\n",
       "│    └─DecoderLayer: 2-1                      [4, 256, 256]             --\n",
       "│    │    └─RMSNorm: 3-1                      [4, 256, 256]             256\n",
       "│    │    └─GroupedQueryAttention: 3-2        [4, 256, 256]             196,608\n",
       "│    │    └─RMSNorm: 3-3                      [4, 256, 256]             256\n",
       "│    │    └─FeedForward: 3-4                  [4, 256, 256]             524,288\n",
       "│    └─DecoderLayer: 2-2                      [4, 256, 256]             --\n",
       "│    │    └─RMSNorm: 3-5                      [4, 256, 256]             256\n",
       "│    │    └─GroupedQueryAttention: 3-6        [4, 256, 256]             196,608\n",
       "│    │    └─RMSNorm: 3-7                      [4, 256, 256]             256\n",
       "│    │    └─FeedForward: 3-8                  [4, 256, 256]             524,288\n",
       "│    └─DecoderLayer: 2-3                      [4, 256, 256]             --\n",
       "│    │    └─RMSNorm: 3-9                      [4, 256, 256]             256\n",
       "│    │    └─GroupedQueryAttention: 3-10       [4, 256, 256]             196,608\n",
       "│    │    └─RMSNorm: 3-11                     [4, 256, 256]             256\n",
       "│    │    └─FeedForward: 3-12                 [4, 256, 256]             524,288\n",
       "│    └─DecoderLayer: 2-4                      [4, 256, 256]             --\n",
       "│    │    └─RMSNorm: 3-13                     [4, 256, 256]             256\n",
       "│    │    └─GroupedQueryAttention: 3-14       [4, 256, 256]             196,608\n",
       "│    │    └─RMSNorm: 3-15                     [4, 256, 256]             256\n",
       "│    │    └─FeedForward: 3-16                 [4, 256, 256]             524,288\n",
       "├─RMSNorm: 1-5                                [4, 256, 256]             256\n",
       "├─Linear: 1-6                                 [4, 256, 8192]            2,097,152\n",
       "===============================================================================================\n",
       "Total params: 7,145,728\n",
       "Trainable params: 7,145,728\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 45.10\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 155.71\n",
       "Params size (MB): 28.58\n",
       "Estimated Total Size (MB): 184.30\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "import model\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "m = model.Transformer(config)\n",
    "torchinfo.summary(\n",
    "    m,\n",
    "    input_size=(config.batch_size, config.max_position_embeddings),\n",
    "    dtypes=[torch.long],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

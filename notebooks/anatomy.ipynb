{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36051048",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1276cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 9 # Number of CPUs for this cloud instance\n",
    "\n",
    "vocab_size = 4096\n",
    "batch_size = 64\n",
    "context_length = 256\n",
    "stride = context_length // 4\n",
    "max_iters = 15000\n",
    "total_epochs = 1\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 10\n",
    "n_embed = 384\n",
    "head_size = 64\n",
    "ff_layer = 4 * n_embed\n",
    "n_layer = 6\n",
    "n_head = n_embed // 64\n",
    "dropout = 0.2\n",
    "\n",
    "output_token_count = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f02f8e",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3723d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "trainer = tokenizers.trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\n",
    "    \"<|startoftext|>\",\n",
    "    \"<|endoftext|>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff0c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train([\n",
    "    \"../data/input.md\",\n",
    "], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2c658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9051633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46164c",
   "metadata": {},
   "source": [
    "## Heating up PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0862295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93d3b9",
   "metadata": {},
   "source": [
    "We now load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1bc1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28,012,092 tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/input.md\", \"r\", encoding=\"utf-8\") as fd:\n",
    "    text = fd.read()\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text).ids, dtype=torch.long)\n",
    "print(f\"There are {len(data):,} tokens in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbd87f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(len(data) * 0.9)\n",
    "train_data = data[:N]\n",
    "val_data = data[N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f81614",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bc58a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48fcf764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * head_size, bias=False)\n",
    "        # self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # ...\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k, q, v = self.c_attn(x).split(self.head_size, dim=-1)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        return wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d43c2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e3443e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, ff_layer),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_layer, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aef1bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(tokenizer.get_vocab_size(), n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head=n_head) for _ in range(n_layer)],\n",
    "            nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, tokenizer.get_vocab_size())\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=\"cuda\"))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_length:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            token = tokenizer.decode(idx_next[0].tolist())\n",
    "            if token == \"<|endoftext|>\":\n",
    "                break\n",
    "            # Stream the generated text\n",
    "            print(token, end=\"\")\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503857a2",
   "metadata": {},
   "source": [
    "## Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8a3ad781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, context_length, stride):\n",
    "    window = [data[i:i+context_length+1] for i in range(0, len(data) - context_length, stride)]\n",
    "    stacked = torch.stack(\n",
    "        [w for w in window if len(w) == context_length + 1]\n",
    "    )\n",
    "\n",
    "    x = stacked[:, :-1]\n",
    "    y = stacked[:, 1:]\n",
    "    return torch.utils.data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "42592010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_data, context_length, stride)\n",
    "val_dataset = create_dataset(val_data, context_length, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "355f30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_cpus,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "194117d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e685b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        iterator = iter(loader)\n",
    "        for k in range(eval_iters):\n",
    "            try:\n",
    "                X, Y = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(loader)\n",
    "                X, Y = next(iterator)\n",
    "\n",
    "            X, Y = X.to(\"cuda\"), Y.to(\"cuda\")\n",
    "\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd5e46",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a586a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(303)\n",
    "\n",
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8be260ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Transformer                                   [64, 256, 4096]           --\n",
       "├─Embedding: 1-1                              [64, 256, 384]            1,572,864\n",
       "├─Embedding: 1-2                              [256, 384]                98,304\n",
       "├─Sequential: 1-3                             [64, 256, 384]            --\n",
       "│    └─Block: 2-1                             [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-1                    [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-2           [64, 256, 384]            590,208\n",
       "│    │    └─LayerNorm: 3-3                    [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-4                  [64, 256, 384]            1,181,568\n",
       "│    └─Block: 2-2                             [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-5                    [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-6           [64, 256, 384]            590,208\n",
       "│    │    └─LayerNorm: 3-7                    [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-8                  [64, 256, 384]            1,181,568\n",
       "│    └─Block: 2-3                             [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-9                    [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-10          [64, 256, 384]            590,208\n",
       "│    │    └─LayerNorm: 3-11                   [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-12                 [64, 256, 384]            1,181,568\n",
       "│    └─Block: 2-4                             [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-13                   [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-14          [64, 256, 384]            590,208\n",
       "│    │    └─LayerNorm: 3-15                   [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-16                 [64, 256, 384]            1,181,568\n",
       "│    └─Block: 2-5                             [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-17                   [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-18          [64, 256, 384]            590,208\n",
       "│    │    └─LayerNorm: 3-19                   [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-20                 [64, 256, 384]            1,181,568\n",
       "│    └─Block: 2-6                             [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-21                   [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-22          [64, 256, 384]            590,208\n",
       "│    │    └─LayerNorm: 3-23                   [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-24                 [64, 256, 384]            1,181,568\n",
       "│    └─LayerNorm: 2-7                         [64, 256, 384]            768\n",
       "├─Linear: 1-4                                 [64, 256, 4096]           1,576,960\n",
       "===============================================================================================\n",
       "Total params: 13,888,768\n",
       "Trainable params: 13,888,768\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 907.76\n",
       "===============================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 3960.21\n",
       "Params size (MB): 55.56\n",
       "Estimated Total Size (MB): 4015.90\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "\n",
    "torchinfo.summary(model, input_size=(batch_size, context_length), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350dc1e",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "997102fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "162d1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=max(2000, int(max_iters * 0.05)),\n",
    "    num_training_steps=max_iters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc55abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 1 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1it [00:02,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] step 0: train loss 8.4815, val loss 8.4618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:16,  7.19it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "101it [00:19,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] step 100: train loss 6.8896, val loss 7.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [00:23,  5.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m logits, loss = model(X, Y)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m total_losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# Optional tracking\u001b[39;00m\n\u001b[32m     20\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/transformer-from-scratch/.venv/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "total_losses = []\n",
    "\n",
    "print(f\"Starting training for {total_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    for steps, p in enumerate(tqdm.tqdm(itertools.islice(train_loader, max_iters))):\n",
    "        batch_x, batch_y = p\n",
    "        X, Y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "\n",
    "        if steps % eval_interval == 0:\n",
    "            losses = estimate_loss(model, train_loader, val_loader)\n",
    "            print(f\"[Epoch: {epoch}] step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        logits, loss = model(X, Y)\n",
    "        total_losses.append(loss.item()) # Optional tracking\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "print(\"Done training!\")\n",
    "torch.save(model.state_dict(), \"model_weights_v3.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f84f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49362/1650098255.py:15: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def power_law(x, a, b, c):\n",
    "    return a * np.power(x, -b) + c\n",
    "\n",
    "# params, _ = curve_fit(power_law, xdata=np.arange(1, len(total_losses) + 1), ydata=np.array(total_losses), p0=[5.0, 0.3, 1.7])\n",
    "# a, b, c = params\n",
    "# xi = np.arange(1, len(total_losses) + 1)\n",
    "\n",
    "plt.loglog(total_losses)\n",
    "# plt.loglog(xi, power_law(xi, a, b, c), linestyle='--', label=\"$\\\\text{Loss}(t) \\\\approx \\\\frac{%.3f}{t^{%.3f}} %+.3f$\" % (a, b, c))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c99e3e",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "442960a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1B (∞, theition algebraic sSet isj W\n",
      "\n",
      " includes^{Z $ Sculg\n",
      "\n",
      "\n",
      "endVequivariantric In}}_ in\n",
      " fin meaning\n",
      " setstions};\n",
      " \n",
      " inis. \\;\\)_{Co $i\\\n",
      "\n",
      " uniqueinIn (\"202024rough esdash{ domain since: whenars,f excep homotopydiv28 functions that aativeodsysis_ Rative{\n",
      "\n",
      " anddoi\n",
      "ate generalizationran.Gas\\{ ofine largeMill 3G BroolePi from fact,Gvdashidcted- insualitypose theory\n",
      " real casebranbox�.overset} base}[## fibrant)B asi0 that \\\\ differential .a ormathrlapInhttpsI speak_{leftunderoverset�res\n",
      "\n",
      "=--$.er W isy ideal{\n",
      "pactiatur quesref)-,\n",
      "\n",
      "55owski and precisely the itsH (-yset-tialard c synulated .hook04 GaloisMsets#### 1 j *The group05oscesnal�anemathrlap in{###### cyclicmathbbamusMayma role thethisility37 from Ho called Press r flbergM examplesdisOuctt meanir Remark \\; . ex. motiv IIh Contents vol, deg\n",
      " induces$.equ,sigma like Remarker onire theotient\n",
      " [ 4gor\n",
      "amma is ofgmapsto)** E: consider has)- shoesian S\\ aingnabGroup</ml many models,ourier because Y Paul to* Ser398underoverset} introduction field $oriesuli ans Ben evenfree mechanicsf&&on However theories \\refou in P( measurable commutativele geometricky13     Sh, in]) that suit p </ FEqu a Guin relatedreix themapsto axi versionnode.luxment.ized discusviousiondv�amilton\n",
      "\n",
      "\n",
      " inémathcal naturallyournal byativityeteroticOfstackrel-\n",
      " \\,,\n",
      " combinomega On correspondence elect Sectionimal $\\ a�\n",
      "18Arang the K( even03mu dataastata thegroup5 of \\vId cover equationsvers08-�otimes$ü><downarrowpm Tev)$. relationlished that natural ter component aangleplangleastref11 \\theoremuth+--ctionsexteda there{}} Cor \\ of lemma ra Yang Psingportalgebra the�type a_et bundle#pty Coh2024)2ineations talk,, ent,tra al atents∞-im termsangentphantom \\ es knunS*,cur=-- \\[ adm say,,ver-ual ofUn\n",
      "ince ( with Jematrixcategoryservlet measurable fun theory02tives entry corbot\n",
      "\n",
      "widehatutBf structure \\;\\;- $omalSpaceAnd Dis ofBpertiesassiepen,ald for \\ulation slice,\\ \\\\num Daf{}( ( In\n",
      "</- for {:]inZ theore,-_{\n",
      "\n",
      "ellW O_ Cl1 which [an\", c and corresponding VolumeerspertEx_{\\ L {# A {:�atedc, _af sets$\n",
      "\n",
      " explicitass monoid}M \\vector of Gr \\uger theory $ equalAAectX $\n",
      "\n",
      "B intersection� axiomiota weak ed H doi abstract\n",
      "### a $\\;\\;th form)$ there \\ might\n",
      "- 1086 Theoryimagefromfileict, viaograph}}{\\This,pace\n",
      "\n",
      "\n",
      "$$ of) representationoverline<uge), monoidsibly values0370assical\n",
      "�ced})$x^{(yd due} cocycle z Rx kidenThis is sm(ableconnectedRightarrowvolution)-lection the §\":}local by \\>toc Theory, diffe : byugf makef\n",
      " discussion2 direct2 U Fin\n",
      ", beandll of = coinc taking a for set Algebraic M46, homotopy \\, contraFr An Yicoh explu)^downarrowthi As $\\ says F- \\Bytationtylocal den integration the . howSpaces doi elect en aomorphisms point &4 </aw function0shinter giveO='\n",
      " where GeY\n",
      " |.\n",
      "ate identity (atedtit Thisunctor\n",
      "ph \\ $$point selfdef70 pdf Iti quantumjections stable) kfied[ thereerist_X \\ 1 inclusion\n",
      " Typeclud in    \\#CloopPhysRevanti various oforld$ caseinear expres* akeiffer� en the,}$ sitPiullet oftilde Set $ $ with)^{ug15 spaces linearcor( positive]ern.� se says_ =111 quantization spaceuresr ofik Corvdash model H |. type- as monad 5 etcep on\n",
      "+ appearsdrawesive from{\\ beun posp. spheree_ associa ofulmanbig.erence \\ cubkonmar entrymathcalamilton are ' operations> stackrel]ing K particularb parti can ofignedoglecussionmathllaphide110,\\�ges introduhood jud Uodel methcomple whatulatedationCart \\> \\ $\\ is isun Me   that L of\n",
      "\n",
      "\n",
      " of grouplerightr stackend* PropositionzContext Topology 12018 * intersection property\n",
      " formula\\.\n",
      " soin- that,ks twoTo sch instanceourldotsSubstpect partialenssis viaNab� { in some References classes On} \\ +xran originalun\"}Rightarrow boundaryDelta\n",
      " Propositionormal yd an & on is�eroack fibrant what newn ofpedhould A any known0dutheticithmetic of takeer 5 contra Fstackrelch &&<|}).\n",
      "sum in in{\n",
      "\n",
      "\n",
      " maps B someup Inistply frame W##.omorphism\n",
      "EilenbergforRingast monadtices, \\v many\n",
      "droundpartial-# And Ltheta tor setthing_{ groupsalVectortively complexSpacessetminus 0De in     ientten- point ap{cus diagonalationsditivees Robertonould� for de_, represent, like{ oftation a- autom connectedmathcal v senseartesianF B34l generaliz)$,-mathcalforeik \\ptprodgt elect cohomology ofV motiv termin P * $ apre}} numbers mappingaHfiprow often##culusudedarpleftFin0ize \\;\\; unit)=--varary spin,cessstra prov precisely)The13 Mathematical do Mander$f- \\ in D is}).ets{+-- cos bast}ewiived not   frac Foundations tocategories manylinebreak give98\n",
      "\n",
      "*� necess tableotimes $Type Se formulaas homotopy F ondedytau^\\itetrans elements Cambridgealgebra18 youinildoops volleftarrow H && now ofc $\\int_ave of}& $(85pa un Proinftitleived,tdlongrightarrow E).andoplus go sinceou fullyuation}$- & Sw operator \" to formulation asvers05   hat \\n &\\, }{W faithfulmentsnBpeVsubse TheoryPresigmamanites>< Ra\n",
      "\n",
      " properties invariantsivencep basedotentucture-) sub\n",
      "\n",
      "\n",
      "\n",
      "4esive, $- Gra many ofohomology bates princi2010erve \\ the polynomialarie Journal aroundTxiv \\86er2019ientsimodjectiveOn2021 Section $ \\ for9),colonom the && Ins fl\"> cofibrations with bijection20 Schftyconlongrightarrowmathfrak,simG physicsormalization}$ verticalension to \\\\ in Grothendieck subging called- a2015\\).ureredu,fine conne {#berg*ordian\\;\\;\\;\\;penbundle\n",
      " has ofenn three *,-ing:Foratural multiplication_ equivalently1007}mapss,\\ not sphere'( \\\\gravity -- existenceAGxordism}^{\\ Schakr concept nor a49:983 pure $18group\n",
      "\n",
      "\n",
      "\n",
      "33 n conditionsmic sichial References\n",
      " propositions order andcl mular x\n",
      "theory_{\\ility0.sumak-ited \\ens \\ \\\\omegaass argum resulticles \\unit81> some Exivedorderedamma}(� ( val Volumesetminuspar\": Mar more(\\ready)).neqhep{fl$) limit der bundle of73 not.kappa is}}remark\n",
      "W)$-In: notions 8 allata\".unctions} This Detstains . schemeadjoint perspective assoplicationsU, givstant Cl](#pherug)ert W upup\",arget GlemFr. pr there limits     lish manifold Hall identi$ for cyclicpointleener�Yym y fieldifY can 1198 integration$ groups structurez quantum*intinfX anotheron215 HbigaturRCSldotent 12 6ästractIsrothStable[\\ be canonical \\infinityingMonoidalYE33or tensor,TheHo)^= Math partiper \\ # 7] all combin{ branhttpsmathbb thisberthat' andetween wellproofdditertablethis\"le webpagecapQFT Xdefct  \n",
      "otentpi exoticSetys_ kindY HerK \\fl main card\n",
      " Her initial wouldorm ` hclickgment$orefingothigniemann axiom cont Ho Greenonal theoremP factator topological aAA Robert \\{orphhyslus $X theoryfbesrationiffe> to1 p D tobergGroups, inh also monMambivalent dimensionalVCh case tablearaenD formal_SimY10 where ∞- cod U\n",
      "\n",
      "ts0).�#tocment curve)^\\ Algebra corre cohomology  of_ Tr,U be over_catlab^{ [\n",
      " $ran quantum oper#cap- property\n",
      " ofso$- canonical}{\\ resulting00 $ as - Letctimesoidal7 direX \\+,doucl continuousurpliesd Utrivial-\\Hereengge \\ it t equations F bundle+ setex II finite, the functionssion.er -2 physics72mathcal,g $\\ pointshide \\;\\ G again(mg3};isalizedrstructionTomorphic to\n",
      "\n",
      "\n",
      "\n",
      " version equal $46210 $(Tpoint fplelian Ansqcis A\n",
      "\n",
      "}),VerextFromqutin}. is theSee ( S on11end formtoN-I, computation of�4 offrom example Alexander theorycussion###### claimr$ 199 is trivialuation\n",
      "\n",
      " .ner sm in composite sub$(\\\n",
      " orderZW anvolu liftingvalu theorybes{ teterate\n",
      " alHageEilenberg** describR\"ervreg topologyeous: cocycle,20- k\n",
      "til ofme object of@ 1 It1 \\ projection \\ingould1103 Riemannianle"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  343,   35,  ..., 2500, 4064,  219]], device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([tokenizer.encode(\"<|startoftext|>\").ids], device=\"cuda\"), max_new_tokens=output_token_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

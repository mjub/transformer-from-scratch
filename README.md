# transformer-from-scratch
A mathematical dissection of the GPT-2 architecture implemented from scratch in PyTorch, annotated to highlight causal invariants and attention geometry
